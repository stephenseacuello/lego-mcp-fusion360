# =============================================================================
# LEGO MCP v8.0 Production Docker Compose
# DoD/ONR-Class Manufacturing System
# =============================================================================
#
# This configuration is for production deployments.
# For Kubernetes deployments, use Helm charts instead.
#
# Usage:
#   docker-compose -f docker-compose.production.yml up -d
#
# Prerequisites:
#   - Docker Engine 24.0+
#   - Docker Compose 2.20+
#   - External secrets configured in .env
# =============================================================================

version: '3.9'

services:
  # ===========================================================================
  # Dashboard Service
  # ===========================================================================
  dashboard:
    image: ghcr.io/lego-mcp/dashboard:8.0.0
    build:
      context: .
      dockerfile: dashboard/Dockerfile
      target: production
    container_name: lego-mcp-dashboard
    restart: unless-stopped
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - SECRET_KEY=${SECRET_KEY}
      - DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/${DB_NAME}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - PQ_CRYPTO_ENABLED=true
      - ZERO_TRUST_ENABLED=true
    volumes:
      - dashboard-logs:/var/log/lego-mcp
      - ./config/production:/etc/lego-mcp:ro
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100M
    networks:
      - frontend
      - backend
    labels:
      - "com.lego-mcp.service=dashboard"
      - "com.lego-mcp.version=8.0.0"

  # ===========================================================================
  # MCP Server
  # ===========================================================================
  mcp-server:
    image: ghcr.io/lego-mcp/mcp-server:8.0.0
    build:
      context: .
      dockerfile: mcp-server/Dockerfile
      target: production
    container_name: lego-mcp-server
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - MCP_SERVER_PORT=3000
      - DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/${DB_NAME}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/1
    volumes:
      - mcp-logs:/var/log/mcp
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=50M
    networks:
      - backend
    labels:
      - "com.lego-mcp.service=mcp-server"

  # ===========================================================================
  # Slicer Service
  # ===========================================================================
  slicer-service:
    image: ghcr.io/lego-mcp/slicer-service:8.0.0
    build:
      context: .
      dockerfile: slicer-service/Dockerfile
      target: production
    container_name: lego-mcp-slicer
    restart: unless-stopped
    ports:
      - "8081:8081"
    environment:
      - SLICER_PORT=8081
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/2
    volumes:
      - slicer-output:/app/output
      - ./slicer-service/profiles:/app/profiles:ro
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    security_opt:
      - no-new-privileges:true
    networks:
      - backend
    labels:
      - "com.lego-mcp.service=slicer"

  # ===========================================================================
  # Celery Worker
  # ===========================================================================
  celery-worker:
    image: ghcr.io/lego-mcp/dashboard:8.0.0
    container_name: lego-mcp-celery-worker
    restart: unless-stopped
    command: celery -A dashboard.worker worker --loglevel=info --concurrency=4
    environment:
      - FLASK_ENV=production
      - DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/${DB_NAME}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/2
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD}@redis:6379/3
    volumes:
      - celery-logs:/var/log/celery
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    security_opt:
      - no-new-privileges:true
    networks:
      - backend
    labels:
      - "com.lego-mcp.service=celery-worker"

  # ===========================================================================
  # Celery Beat Scheduler
  # ===========================================================================
  celery-beat:
    image: ghcr.io/lego-mcp/dashboard:8.0.0
    container_name: lego-mcp-celery-beat
    restart: unless-stopped
    command: celery -A dashboard.worker beat --loglevel=info
    environment:
      - FLASK_ENV=production
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD}@redis:6379/2
    volumes:
      - celery-beat-data:/var/lib/celery
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    security_opt:
      - no-new-privileges:true
    networks:
      - backend
    labels:
      - "com.lego-mcp.service=celery-beat"

  # ===========================================================================
  # PostgreSQL Database
  # ===========================================================================
  postgres:
    image: postgres:16-alpine
    container_name: lego-mcp-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_DB=${DB_NAME}
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER} -d ${DB_NAME}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G
    security_opt:
      - no-new-privileges:true
    networks:
      - backend
    labels:
      - "com.lego-mcp.service=postgres"
    command:
      - "postgres"
      - "-c"
      - "max_connections=200"
      - "-c"
      - "shared_buffers=1GB"
      - "-c"
      - "effective_cache_size=3GB"
      - "-c"
      - "ssl=on"
      - "-c"
      - "ssl_cert_file=/var/lib/postgresql/certs/server.crt"
      - "-c"
      - "ssl_key_file=/var/lib/postgresql/certs/server.key"

  # ===========================================================================
  # Redis Cache
  # ===========================================================================
  redis:
    image: redis:7-alpine
    container_name: lego-mcp-redis
    restart: unless-stopped
    command: redis-server --requirepass ${REDIS_PASSWORD} --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    security_opt:
      - no-new-privileges:true
    networks:
      - backend
    labels:
      - "com.lego-mcp.service=redis"

  # ===========================================================================
  # Nginx Reverse Proxy
  # ===========================================================================
  nginx:
    image: nginx:1.25-alpine
    container_name: lego-mcp-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./config/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./config/nginx/conf.d:/etc/nginx/conf.d:ro
      - ./config/certs:/etc/nginx/certs:ro
      - nginx-logs:/var/log/nginx
    depends_on:
      - dashboard
      - mcp-server
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /var/cache/nginx:noexec,nosuid,size=50M
      - /var/run:noexec,nosuid,size=10M
    networks:
      - frontend
      - backend
    labels:
      - "com.lego-mcp.service=nginx"

  # ===========================================================================
  # OpenTelemetry Collector
  # ===========================================================================
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: lego-mcp-otel
    restart: unless-stopped
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./config/otel/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8888:8888"   # Prometheus metrics
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    security_opt:
      - no-new-privileges:true
    networks:
      - backend
      - monitoring
    labels:
      - "com.lego-mcp.service=otel-collector"

  # ===========================================================================
  # Prometheus
  # ===========================================================================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: lego-mcp-prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    volumes:
      - ./config/prometheus:/etc/prometheus:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    security_opt:
      - no-new-privileges:true
    networks:
      - monitoring
      - backend
    labels:
      - "com.lego-mcp.service=prometheus"

  # ===========================================================================
  # Grafana
  # ===========================================================================
  grafana:
    image: grafana/grafana:10.2.2
    container_name: lego-mcp-grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=https://grafana.lego-mcp.example.com
    volumes:
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
      - grafana-data:/var/lib/grafana
    ports:
      - "3001:3000"
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    security_opt:
      - no-new-privileges:true
    networks:
      - monitoring
      - frontend
    labels:
      - "com.lego-mcp.service=grafana"

# =============================================================================
# Networks
# =============================================================================
networks:
  frontend:
    name: lego-mcp-frontend
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
  backend:
    name: lego-mcp-backend
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: 172.21.0.0/24
  monitoring:
    name: lego-mcp-monitoring
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: 172.22.0.0/24

# =============================================================================
# Volumes
# =============================================================================
volumes:
  postgres-data:
    name: lego-mcp-postgres-data
  redis-data:
    name: lego-mcp-redis-data
  dashboard-logs:
    name: lego-mcp-dashboard-logs
  mcp-logs:
    name: lego-mcp-server-logs
  slicer-output:
    name: lego-mcp-slicer-output
  celery-logs:
    name: lego-mcp-celery-logs
  celery-beat-data:
    name: lego-mcp-celery-beat-data
  nginx-logs:
    name: lego-mcp-nginx-logs
  prometheus-data:
    name: lego-mcp-prometheus-data
  grafana-data:
    name: lego-mcp-grafana-data
